{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b104d21",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ff36e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-google-genai langchain-pinecone pinecone python-dotenv langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a338042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY cargada: True\n",
      "PINECONE_API_KEY cargada: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"GOOGLE_API_KEY cargada:\", \"GOOGLE_API_KEY\" in os.environ)\n",
    "print(\"PINECONE_API_KEY cargada:\", \"PINECONE_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71798e9",
   "metadata": {},
   "source": [
    "### Document upload from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db64bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento cargado: 1 p치gina(s)\n",
      "Total de caracteres: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Documento cargado: {len(docs)} p치gina(s)\")\n",
    "print(f\"Total de caracteres: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17c895",
   "metadata": {},
   "source": [
    "### Separate in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef106cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 63\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"Total de chunks creados: {len(splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f100445",
   "metadata": {},
   "source": [
    "### Create Embeddings with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3dba515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "for model in client.models.list():\n",
    "    if \"embedContent\" in (model.supported_actions or []):\n",
    "        print(model.name, model.supported_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d3405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi칩n del embedding: 3072\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Test embedding\n",
    "test = embeddings.embed_query(\"test de embedding\")\n",
    "print(f\"Dimensi칩n del embedding: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31243fb5",
   "metadata": {},
   "source": [
    "### Save in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b980bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadline3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyreadline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf462397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos guardados en Pinecone correctamente\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "INDEX_NAME = \"rag-index\"\n",
    "INDEX_DIMENSION = 3072\n",
    "\n",
    "def _index_names(index_list):\n",
    "    if hasattr(index_list, \"names\"):\n",
    "        return list(index_list.names())\n",
    "    names = []\n",
    "    for item in index_list:\n",
    "        if isinstance(item, dict) and \"name\" in item:\n",
    "            names.append(item[\"name\"])\n",
    "        else:\n",
    "            names.append(item)\n",
    "    return names\n",
    "\n",
    "existing_indexes = _index_names(pc.list_indexes())\n",
    "if INDEX_NAME in existing_indexes:\n",
    "    info = pc.describe_index(INDEX_NAME)\n",
    "    existing_dim = getattr(info, \"dimension\", None)\n",
    "    if existing_dim is None and isinstance(info, dict):\n",
    "        existing_dim = info.get(\"dimension\")\n",
    "    if existing_dim != INDEX_DIMENSION:\n",
    "        pc.delete_index(INDEX_NAME)\n",
    "        while INDEX_NAME in _index_names(pc.list_indexes()):\n",
    "            time.sleep(1)\n",
    "\n",
    "if INDEX_NAME not in _index_names(pc.list_indexes()):\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=INDEX_DIMENSION,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(INDEX_NAME).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n",
    "vector_store.add_documents(splits)\n",
    "\n",
    "print(\"Documentos guardados en Pinecone correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e29a18",
   "metadata": {},
   "source": [
    "### Create the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854520ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks recuperados: 3\n",
      "\n",
      "Primer chunk:\n",
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Test\n",
    "result = retriever.invoke(\"What is an AI agent?\")\n",
    "print(f\"Chunks recuperados: {len(result)}\")\n",
    "print(\"\\nPrimer chunk:\")\n",
    "print(result[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ef679",
   "metadata": {},
   "source": [
    "### Create LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25460a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36054ad4",
   "metadata": {},
   "source": [
    "### Build the RAG Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b925dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain creada correctamente\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following retrieved context to answer the question. \n",
    "If you don't know the answer, say that you don't know.\n",
    "Keep the answer concise and clear.\n",
    "\n",
    "Context: {context}\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain creada correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614903c",
   "metadata": {},
   "source": [
    "### Ask questions to the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f356881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is an AI agent?\n",
      "A: An AI agent, as described in the context, is a system that uses a Large Language Model (LLM) as its core controller or \"brain.\" This LLM is complemented by several key components:\n",
      "\n",
      "*   **Planning:** To break down large tasks into subgoals and refine actions through self-criticism.\n",
      "*   **Memory:** Including short-term in-context learning and long-term retention via external stores.\n",
      "*   **Tool use:** To call external APIs for additional information, code execution, or access to proprietary data.\n",
      "\n",
      "Q: What are the main components of an AI agent?\n",
      "A: The main components of an AI agent are:\n",
      "\n",
      "*   **Planning**: Decomposing complex tasks into smaller steps (e.g., Chain of Thought, Tree of Thoughts).\n",
      "*   **Memory**: Includes short-term (in-context learning) and long-term memory (external vector store, memory stream, retrieval model).\n",
      "*   **Tool use**: Calling external APIs for additional information or capabilities.\n",
      "*   **Reflection mechanism**: Synthesizing memories into higher-level inferences to guide future behavior.\n",
      "\n",
      "Q: What is chain of thought prompting?\n",
      "A: Chain of Thought (CoT) prompting is a standard technique that instructs a language model to \"think step by step\" to enhance its performance on complex tasks. It allows the model to utilize more test-time computation by decomposing difficult tasks into smaller, simpler steps, thereby transforming big tasks into multiple manageable ones and providing insight into the model's thinking process.\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What is an AI agent?\"\n",
    "answer1 = rag_chain.invoke(question1)\n",
    "print(f\"Q: {question1}\")\n",
    "print(f\"A: {answer1}\")\n",
    "print()\n",
    "\n",
    "question2 = \"What are the main components of an AI agent?\"\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "print(f\"Q: {question2}\")\n",
    "print(f\"A: {answer2}\")\n",
    "print()\n",
    "\n",
    "question3 = \"What is chain of thought prompting?\"\n",
    "answer3 = rag_chain.invoke(question3)\n",
    "print(f\"Q: {question3}\")\n",
    "print(f\"A: {answer3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
